{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlT6adHIDyesu1ZCY1eBlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themodernturing/pakistan-penal-code-qa/blob/main/pakistan_panel_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üìù Introduction\n",
        "This Python notebook is designed to build an AI-powered question-answering system using the Pakistan Penal Code as a primary source of information. The goal is to allow users to ask legal questions and receive accurate, context-aware answers based on the contents of the Penal Code.\n",
        "\n",
        "The notebook combines various powerful tools and techniques from modern machine learning and natural language processing (NLP), including:\n",
        "\n",
        "##üîç PDF Text Extraction\n",
        "Using PyMuPDF (fitz), the notebook extracts the full text content from the Pakistan Penal Code PDF document. This allows us to convert the legal text into a machine-readable format for downstream processing.\n",
        "\n",
        "##‚úÇÔ∏è Text Chunking\n",
        "Since legal documents are often long and detailed, the full text is split into smaller, overlapping chunks using LangChain‚Äôs RecursiveCharacterTextSplitter. This ensures that the language model can effectively process and understand each segment.\n",
        "\n",
        "##üß† Semantic Embeddings & Vector Store\n",
        "The notebook uses HuggingFace sentence-transformers to generate embeddings for each text chunk. These embeddings capture the semantic meaning of the text and are stored in a FAISS vector store for efficient similarity search.\n",
        "\n",
        "##üîÅ Retrieval-Based QA Pipeline\n",
        "Using LangChain‚Äôs RetrievalQA chain, the system retrieves the most relevant sections of the document in response to a user query and uses a pretrained transformer model to generate a natural language answer. This enables a more contextually aware and document-grounded response.\n",
        "\n",
        "##üåê Web Interface\n",
        "With Gradio, a simple and interactive web-based UI can be created, enabling users to input legal queries and receive real-time answers. This makes the system accessible to non-technical users like law students, researchers, and the general public.\n",
        "\n"
      ],
      "metadata": {
        "id": "KoTahM0gc9sd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6EnM89lJcEA"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import fitz  # This works now after installing\n",
        "\n",
        "def extract_text_from_pdf(file_path=\"/content/Pakistan Panel Code.pdf\"):\n",
        "    doc = fitz.open(file_path)\n",
        "    full_text = \"\"\n",
        "    for page_num, page in enumerate(doc, start=1):\n",
        "        text = page.get_text()\n",
        "        full_text += f\"\\n\\n--- Page {page_num} ---\\n\\n{text}\"\n",
        "    return full_text\n",
        "\n",
        "# Usage\n",
        "pdf_text = extract_text_from_pdf()\n",
        "print(pdf_text[:1000])  # Preview\n"
      ],
      "metadata": {
        "id": "NzJ6bEOzKbxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n"
      ],
      "metadata": {
        "id": "JU2GfRLGKjRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Assuming pdf_text holds your full document\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_text(pdf_text)\n",
        "\n",
        "print(f\"Total chunks: {len(chunks)}\")\n",
        "print(chunks[0])  # Preview the first chunk\n"
      ],
      "metadata": {
        "id": "7tCkv0mwLZA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community\n",
        "!pip install -U openai tiktoken faiss-cpu\n",
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "r01BSiySLs8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = FAISS.from_texts(chunks, embedding=embedding_model)\n",
        "\n",
        "# Optional: save the vectorstore\n",
        "vectorstore.save_local(\"pakistan_penal_code_index\")\n",
        "\n"
      ],
      "metadata": {
        "id": "HNus_s29Lz7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load locally using Hugging Face Transformers\n",
        "qa_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=512)\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=qa_pipeline)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "query = \"What is the punishment for theft in the Pakistan Penal Code?\"\n",
        "result = qa_chain({\"query\": query})\n",
        "\n",
        "print(\"Answer:\", result[\"result\"])\n"
      ],
      "metadata": {
        "id": "RpZb86upMTx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "# Step 1: Extract text from PDF\n",
        "def extract_text_from_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    full_text = \"\"\n",
        "    for page_num, page in enumerate(doc, start=1):\n",
        "        text = page.get_text()\n",
        "        full_text += f\"\\n\\n--- Page {page_num} ---\\n\\n{text}\"\n",
        "    return full_text\n",
        "\n",
        "# Step 2: Build everything from PDF\n",
        "def build_qa_system(pdf_path):\n",
        "    print(\"üìÑ Extracting text...\")\n",
        "    full_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    print(\"‚úÇÔ∏è Splitting text...\")\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = splitter.split_text(full_text)\n",
        "\n",
        "    print(\"üß† Embedding & indexing...\")\n",
        "    embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectordb = FAISS.from_texts(chunks, embedding=embedder)\n",
        "\n",
        "    print(\"ü§ñ Loading local LLM...\")\n",
        "    hf_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=512)\n",
        "    llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "    print(\"üîó Creating QA chain...\")\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=vectordb.as_retriever(),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    return qa\n",
        "\n",
        "# Step 3: Interface logic\n",
        "pdf_path = \"/content/Pakistan Panel Code.pdf\"\n",
        "qa_chain = build_qa_system(pdf_path)\n",
        "\n",
        "def answer_question(query):\n",
        "    result = qa_chain({\"query\": query})\n",
        "    return result[\"result\"]\n",
        "\n",
        "# Step 4: Launch Gradio UI\n",
        "demo = gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(label=\"Ask a question about the Pakistan Penal Code\"),\n",
        "    outputs=gr.Textbox(label=\"Answer\"),\n",
        "    title=\"Pakistan Penal Code Chatbot\",\n",
        "    description=\"Ask legal questions based on the Pakistan Penal Code PDF file.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "uDPgAWIhM9uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "id": "nk5KdwaqSRHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mKOTy5tSzIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}